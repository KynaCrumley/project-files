{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e8ecb-1464-40da-b9f7-72769f29555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Sheng with some minor changes by Kyna\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cat\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('Utils')\n",
    "\n",
    "class AgeEncoding(nn.Module):\n",
    "    \"Implement the AE function.\"\n",
    "    def __init__(self, d_model, dropout, out_dim,max_len=240):\n",
    "        super(AgeEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model)).float())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.fc6 = nn.Sequential()\n",
    "        self.fc6.add_module('fc6_s1',nn.Linear(d_model,512))\n",
    "        self.fc6.add_module('lrn0_s1',nn.LayerNorm(512))\n",
    "        self.fc6.add_module('fc6_s3',nn.Linear(512, out_dim))\n",
    "\n",
    "    def forward(self, x, age_id):\n",
    "        '''modification: convert age_id into an integer index that falls within the range of max_len (240)\n",
    "        by rounding and clamping age_id.'''\n",
    "        # Convert age to an index within the range [0, max_len-1]\n",
    "        age_index = age_id.round().clamp(0, self.pe.size(0) - 1).long()\n",
    "        y = self.pe[age_index,:]\n",
    "        y = self.fc6(y)\n",
    "\n",
    "        x += y\n",
    "        return self.dropout(x)\n",
    "       \n",
    "    #def forward(self, x, age_id):\n",
    "        #y = torch.autograd.Variable(self.pe[age_id,:], \n",
    "                         #requires_grad=False)\n",
    "        #y = self.fc6(y)\n",
    "\n",
    "        #x += y\n",
    "        #return self.dropout(x)\n",
    "\n",
    "                         \n",
    "\n",
    "    \n",
    "class AgeEncoding_simple(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, out_dim,max_len=240):\n",
    "        super(AgeEncoding_simple, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.fc6 = nn.Linear(out_dim+1,out_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, age_id):\n",
    "        age_id = (age_id -0)/(240 - 0 + 1e-6)\n",
    "        y = torch.cat([x.float(),age_id.unsqueeze(-1).float()],dim=1)\n",
    "        x = self.fc6(y)\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class NetWork(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel=1,feat_dim=1024,expansion = 4, type_name='conv3x3x3', norm_type = 'Instance'):\n",
    "        super(NetWork, self).__init__()\n",
    "        \n",
    "\n",
    "        self.conv = nn.Sequential()\n",
    "\n",
    "        self.conv.add_module('conv0_s1',nn.Conv3d(in_channel, 4*expansion, kernel_size=1))\n",
    "\n",
    "        if norm_type == 'Instance':\n",
    "           self.conv.add_module('lrn0_s1',nn.InstanceNorm3d(4*expansion))\n",
    "        else:\n",
    "           self.conv.add_module('lrn0_s1',nn.BatchNorm3d(4*expansion))\n",
    "        self.conv.add_module('relu0_s1',nn.ReLU(inplace=True))\n",
    "        self.conv.add_module('pool0_s1',nn.MaxPool3d(kernel_size=3, stride=2))\n",
    "\n",
    "        self.conv.add_module('conv1_s1',nn.Conv3d(4*expansion, 32*expansion, kernel_size=3,padding=0, dilation=2))\n",
    "        \n",
    "        if norm_type == 'Instance':\n",
    "            self.conv.add_module('lrn1_s1',nn.InstanceNorm3d(32*expansion))\n",
    "        else:\n",
    "            self.conv.add_module('lrn1_s1',nn.BatchNorm3d(32*expansion))\n",
    "        self.conv.add_module('relu1_s1',nn.ReLU(inplace=True))\n",
    "        self.conv.add_module('pool1_s1',nn.MaxPool3d(kernel_size=3, stride=2))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.conv.add_module('conv2_s1',nn.Conv3d(32*expansion, 64*expansion, kernel_size=5, padding=2, dilation=2))\n",
    "        \n",
    "        if norm_type == 'Instance':\n",
    "            self.conv.add_module('lrn2_s1',nn.InstanceNorm3d(64*expansion))\n",
    "        else:\n",
    "            self.conv.add_module('lrn2_s1',nn.BatchNorm3d(64*expansion))\n",
    "        self.conv.add_module('relu2_s1',nn.ReLU(inplace=True))\n",
    "        self.conv.add_module('pool2_s1',nn.MaxPool3d(kernel_size=3, stride=2))\n",
    "\n",
    "        \n",
    "        self.conv.add_module('conv3_s1',nn.Conv3d(64*expansion, 64*expansion, kernel_size=3, padding=1, dilation=2))\n",
    "        \n",
    "        if norm_type == 'Instance':\n",
    "            self.conv.add_module('lrn3_s1',nn.InstanceNorm3d(64*expansion))\n",
    "        else:\n",
    "            self.conv.add_module('lrn2_s1',nn.BatchNorm3d(64*expansion))\n",
    "        self.conv.add_module('relu3_s1',nn.ReLU(inplace=True))\n",
    "        self.conv.add_module('pool2_s1',nn.MaxPool3d(kernel_size=5, stride=2))\n",
    "\n",
    "        self.fc6 = nn.Sequential()\n",
    "        self.fc6.add_module('fc6_s1',nn.Linear(64*expansion*5*5*5, feat_dim))\n",
    "        self.age_encoder = AgeEncoding(512,0.1,feat_dim)\n",
    "\n",
    "\n",
    "    def load(self,checkpoint):\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = torch.load(checkpoint)['state_dict']\n",
    "        pretrained_dict = {k[6:]: v for k, v in list(pretrained_dict.items()) if k[6:] in model_dict and 'conv3_s1' not in k and 'fc6' not in k and 'fc7' not in k and 'fc8' not in k}\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        \n",
    "\n",
    "        self.load_state_dict(model_dict)\n",
    "        print([k for k, v in list(pretrained_dict.items())])\n",
    "        return pretrained_dict.keys()\n",
    "\n",
    "    def freeze(self, pretrained_dict_keys):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in pretrained_dict_keys:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "\n",
    "    def save(self,checkpoint):\n",
    "        torch.save(self.state_dict(), checkpoint)\n",
    "    \n",
    "    #def forward(self, x, age_id):\n",
    "\n",
    "        #z = self.conv(x)\n",
    "        #z = self.fc6(z.view(x.shape[0],-1))\n",
    "        #if age_id is not None:\n",
    "            #z = self.age_encoder(z,age_id)\n",
    "        #return z\n",
    "        \n",
    "    def forward(self, x, age_id):\n",
    "        z = self.conv(x)\n",
    "        z = self.fc6(z.view(x.shape[0], -1))\n",
    "\n",
    "        # Check if age_id is not None and use AgeEncoding\n",
    "        if age_id is not None:\n",
    "            z = self.age_encoder(z, age_id)\n",
    "        return z\n",
    "\n",
    "\n",
    "def weights_init(model):\n",
    "    if type(model) in [nn.Conv3d,nn.Linear]:\n",
    "        nn.init.xavier_normal_(model.weight.data)\n",
    "        nn.init.constant_(model.bias.data, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77181f-c8cf-46c3-900b-05a02456e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: RUN AT YOUR OWN RISK, TAKES HALF A DAY ON MACBOOK\n",
    "\n",
    "#Converts DICOM folders to NiFti images with SimpleITK\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Resample image if the spacing isn't consistent\n",
    "def resample_image(image, new_spacing):\n",
    "    original_spacing = image.GetSpacing()\n",
    "    original_size = image.GetSize()\n",
    "\n",
    "    new_size = [\n",
    "        int(round(osz*ospc/nspc)) for osz, ospc, nspc in zip(original_size, original_spacing, new_spacing)\n",
    "    ]\n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetOutputSpacing(new_spacing)\n",
    "    resampler.SetSize(new_size)\n",
    "    resampler.SetTransform(sitk.Transform())\n",
    "    resampler.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "    return resampler.Execute(image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_artifacts(image):\n",
    "'''remove artifacts for an image proccessed.\n",
    "This is assuming the artifact has a significantly different intensity than the brain,\n",
    "which is not be really the case'''\n",
    "    \n",
    "    # we can calculate the threshold using Otsu's method\n",
    "    otsu_filter = sitk.OtsuThresholdImageFilter()\n",
    "    otsu_filter.Execute(image)\n",
    "    threshold = otsu_filter.GetThreshold()\n",
    "    \n",
    "    # Create an image filled with the threshold value + make sure to match the size and pixel type\n",
    "    threshold_image = sitk.Image(image.GetSize(), image.GetPixelIDValue())\n",
    "    threshold_image.CopyInformation(image)\n",
    "    threshold_image += threshold  # Broadcasting the threshold value to all pixels\n",
    "    \n",
    "    #make a binary 'mask' where true values indicate the presence of the brain\n",
    "    brain_mask = sitk.Greater(image, threshold_image)\n",
    "    \n",
    "    #Apply the mask to the image\n",
    "    image = sitk.Mask(image, brain_mask, outsideValue=0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_dicom_to_nifti(dicom_folder, output_folder, target_spacing=1.2):\n",
    "\n",
    "    \n",
    "    #Make a list of all DICOM file paths in the folder\n",
    "    dicom_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(dicom_folder)\n",
    "    \n",
    "    # Exclude the first 15 and the last 15 images (some files start and end with 15 slices of random noise, might be what's creating artifacts)\n",
    "    #For simplicity and consistency, do the exclusion for all folders\n",
    "    dicom_names = dicom_names[15:-15]\n",
    "\n",
    "    #Check if there are DICOM files in the folder\n",
    "    if not dicom_names:\n",
    "        print(f\"No DICOM files found in the folder: {dicom_folder}\")\n",
    "        return\n",
    "\n",
    "    #Initialize the reader\n",
    "    reader = sitk.ImageSeriesReader()\n",
    "    reader.SetFileNames(dicom_names)\n",
    "\n",
    "    #Execute the reader (to load the DICOM series)\n",
    "    image = reader.Execute()\n",
    "\n",
    "    # Resample if we need (i.e. if the spacing doesn't match the target spacing)\n",
    "    current_spacing = image.GetSpacing()\n",
    "    if current_spacing[2] != target_spacing: #should i explicitly set target spacing to 1.2? \n",
    "        new_spacing = (current_spacing[0], current_spacing[1], target_spacing)\n",
    "        image = resample_image(image, new_spacing)\n",
    "\n",
    "    #PermuteAxes to change the axes of the data\n",
    "    image = sitk.PermuteAxes(image, [2, 1, 0])\n",
    "\n",
    "    #remove artifacts\n",
    "    #image = remove_artifacts(image)\n",
    "\n",
    "    # Extracting the name for the NIFTI file from one of the DICOM files in the folder (naming is consistent throughout a folder)\n",
    "    sample_name = os.path.basename(dicom_names[0])\n",
    "    parts = sample_name.split('_')\n",
    "    nifti_name = '_'.join(parts[1:4] + [parts[9]] + parts[-1].split('.'))\n",
    "    nifti_name = re.sub(r'\\.dcm$', '', nifti_name) + '.nii.gz'\n",
    "\n",
    "    # Specify the output file path for the nifti\n",
    "    output_file_path = os.path.join(output_folder, nifti_name)\n",
    "\n",
    "    # Write the image to the NIFTI file\n",
    "    sitk.WriteImage(image, output_file_path)\n",
    "\n",
    "    print(f\"Conversion complete. NIFTI file saved as {output_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "#walk through directories, if finds dicom, then take all files in that folder and convert to one nifti\n",
    "def traverse_directories(root_folder, output_folder):\n",
    "    for subdir, dirs, files in os.walk(root_folder):\n",
    "        if any(f.endswith('.dcm') for f in files):\n",
    "            convert_dicom_to_nifti(subdir, output_folder)\n",
    "\n",
    "root_dir = '/Users/kynacrumley/Desktop/Fall2023.nosync/ADNI_DATA/ADNI'\n",
    "output_dir = '//Users/kynacrumley/Desktop/Fall2023.nosync/ADNI_DATA/ADNI_NIITEST'\n",
    "traverse_directories(root_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d561f-ff09-471a-89a8-8c13ec8d0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: RUN AT YOUR OWN RISK, TAKES MULTIPLE HOURS\n",
    "\n",
    "#Evaluates all images in all subdirectories in the root folder through the CNN.\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import skimage.transform as skTrans\n",
    "\n",
    "# Initialize pretrained model\n",
    "device = torch.device('cpu')\n",
    "\n",
    "checkpoint_path = '/Users/kynacrumley/Desktop/Fall2023.nosync/SDS3386/Term_Project/age_expansion_8_model_low_loss.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_state_dict = checkpoint['state_dict']\n",
    "\n",
    "# So that I can run the from models import NetWork as a .py in my terminal\n",
    "module_path = '/Users/kynacrumley/Desktop/Fall2023.nosync/SDS3386/Term_Project'\n",
    "sys.path.append(module_path)\n",
    "from modelsv2 import NetWork\n",
    "\n",
    "model = NetWork(in_channel=3, feat_dim=1024)  # should be 1024\n",
    "\n",
    "# Load the reference sheet\n",
    "ref_df = pd.read_csv('/Users/kynacrumley/Desktop/PTReferenceSheet.csv')\n",
    "\n",
    "# Extract Features from all files in a folder.\n",
    "results = []\n",
    "root_path = '/Users/kynacrumley/Desktop/Fall2023.nosync/ADNI_DATA/ADNI1-3Raw/ADNI_Raw'\n",
    "\n",
    "# Let's keep track of files that aren't processed correctly\n",
    "unprocessed_files = []\n",
    "\n",
    "# Regular expression to extract subject ID\n",
    "id_pattern = re.compile(r'([A-Z]\\d+)_ADNI_')\n",
    "\n",
    "# Traverse directories and process .nii files\n",
    "for subdir, dirs, files in os.walk(root_path):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.nii'):\n",
    "            print(f\"Processing file: {file_name}\")  # Print the file being processed\n",
    "\n",
    "            match = id_pattern.match(file_name)\n",
    "            if match:\n",
    "                try:\n",
    "                    unique_id = match.group(1)  # Unique ID (ex. I100021)\n",
    "\n",
    "                    ref_info = ref_df[ref_df['UniqueID'] == unique_id]\n",
    "                    if not ref_info.empty:\n",
    "                        session_date = ref_info['SessionId'].iloc[0]\n",
    "                        age = ref_info['EstimatedAge'].iloc[0]\n",
    "                        age_tensor = torch.tensor([age], dtype=torch.float32).to(device)\n",
    "                    else:\n",
    "                        # If age is not found, set age_tensor to None\n",
    "                        age_tensor = None\n",
    "                        session_date = \"Unknown\"  # Handle unknown session date\n",
    "\n",
    "                    image_path = os.path.join(subdir, file_name)\n",
    "\n",
    "                    # Load image\n",
    "                    image = nib.load(image_path)\n",
    "                    data = image.get_fdata()\n",
    "                    data = skTrans.resize(data, (3, 100, 100, 100), order=1, preserve_range=True)\n",
    "                    data = (data - data.mean()) / data.std()\n",
    "                    data = torch.tensor(data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                    # Forward pass through the model\n",
    "                    output = model(data, age_id=age_tensor).squeeze().detach().cpu().numpy()\n",
    "\n",
    "                    # Create a dictionary for each image\n",
    "                    image_features = {'UniqueID': unique_id, 'SessionId': session_date}\n",
    "                    for i, value in enumerate(output):\n",
    "                        feature_key = f'Feature_{i + 1}'\n",
    "                        image_features[feature_key] = value\n",
    "\n",
    "                    results.append(image_features)\n",
    "\n",
    "                    print('Processed successfully:', file_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {e}\")\n",
    "                    unprocessed_files.append({'FileName': file_name, 'Error': str(e)})\n",
    "            else:\n",
    "                print(f\"File name pattern does not match: {file_name}\")\n",
    "                unprocessed_files.append({'FileName': file_name, 'Error': 'Pattern Mismatch'})\n",
    "\n",
    "# Create DataFrames from the results and unprocessed files\n",
    "df = pd.DataFrame(results)\n",
    "df_unprocessed = pd.DataFrame(unprocessed_files)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"ADNI123_051223KC_Features_Wide.csv\", index=False)\n",
    "df_unprocessed.to_csv(\"Unprocessed_Files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cbc08-43ed-4b05-b62c-997a0da5679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('ADNI123_051223KC_Features_Wide.csv')\n",
    "meta = pd.read_csv('MPRAGEMETA_02Dec2023.csv')\n",
    "diag = pd.read_csv('DXSUM_PDXCONV_ADNIALL_01Dec2023.csv')\n",
    "\n",
    "#rename columns to match between dataframes\n",
    "meta = meta.rename(columns = {'ImageUID':'UniqueID','ScanDate':'SessionId'})\n",
    "\n",
    "#drop unnecessary columns in meta\n",
    "meta = meta.drop(columns = ['Orig/Proc','Visit','MagStrength','Sequence','StudyID','SeriesID'])\n",
    "\n",
    "#make sure they bot have SessionId in the same format:\n",
    "data = data.drop(columns = [\"SessionId\"]) #seems that the dates don't match -> some dcm file names have dates that are not the same as their enclosing folder (the enclosing folder date matches other metadata such as in mpragemeta.csv)\n",
    "\n",
    "#handle duplicates\n",
    "\n",
    "duplicates = data[data['UniqueID'].duplicated(keep=False)]\n",
    "print(duplicates)\n",
    "\n",
    "data = data.drop_duplicates(subset='UniqueID', keep='first')\n",
    "\n",
    "#make sure they both have UniqueID in the same format (need to add an I to the meta one)\n",
    "meta['UniqueID'] = \"I\" + meta['UniqueID'].astype('string')\n",
    "\n",
    "#inner merge\n",
    "df = data.merge(meta, on=['UniqueID'], how='inner')\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]  # Move the last two columns to the front\n",
    "df = df[cols]\n",
    "\n",
    "#Add diagnosis by matching nearest date\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def find_closest_dates2(df1, df2):\n",
    "    # Ensure the date columns in both dataframes are in datetime format\n",
    "    df1['SessionId'] = pd.to_datetime(df1['SessionId'])\n",
    "    df2['SessionId'] = pd.to_datetime(df2['SessionId'])\n",
    "\n",
    "    # Function to find the closest date for a given subject\n",
    "    def get_closest_date(subject_id, session_date):\n",
    "        # Filter df2 to obtain only rows corresponding to the same subjectId\n",
    "        subject_dates = df2[df2['SubjectID'] == subject_id]['SessionId']\n",
    "\n",
    "        # Check if there are any dates for this subject\n",
    "        if not subject_dates.empty:\n",
    "            # Find the closest date in subject_dates to the session_date\n",
    "            closest_date = min(subject_dates, key=lambda x: abs(x - session_date))\n",
    "            return closest_date\n",
    "        else:\n",
    "            # if no dates available return none\n",
    "            print(\"returning None for \", subject_id, session_date)\n",
    "            return None\n",
    "\n",
    "    # Apply the get_closest_date function to each row in df1\n",
    "    df1['ClosestDate'] = df1.apply(lambda row: get_closest_date(row['SubjectID'], row['SessionId']), axis=1)\n",
    "\n",
    "    return df1\n",
    "\n",
    "#keep only necessary columns\n",
    "diag = diag[['PTID','VISDATE','DXCURREN']]\n",
    "\n",
    "#rename them to match\n",
    "diag = diag.rename(columns = {'PTID':'SubjectID','VISDATE':'SessionId'})\n",
    "result = find_closest_dates2(df, diag)\n",
    "\n",
    "\n",
    "#now we can add the diagnosis column by matching the closest date\n",
    "merged_df = pd.merge(result, diag, left_on='ClosestDate', right_on='SessionId', how='left')\n",
    "merged_df = merged_df.drop(columns = ['SubjectID_y','SessionId_y'])\n",
    "\n",
    "merged_df['scan_ID'] = merged_df['SubjectID_x'].str.replace('_','')\n",
    "\n",
    "features = [f'Feature_{i}' for i in range(1, 1025)]\n",
    "df_unique = merged_df.drop_duplicates(subset=features)\n",
    "\n",
    "#Add source column\n",
    "\n",
    "test_df = pd.read_csv('Test_diagnosis_ADNI.tsv', sep = '\\t')\n",
    "train_df = pd.read_csv('Train_diagnosis_ADNI.tsv', sep = '\\t')\n",
    "val_df = pd.read_csv('Val_diagnosis_ADNI.tsv', sep = '\\t')\n",
    "\n",
    "train_df['source'] = 'train'\n",
    "test_df['source'] = 'test'\n",
    "val_df['source'] = 'val'\n",
    "\n",
    "source_df = pd.concat([train_df, test_df, val_df], ignore_index=True)\n",
    "source_df = source_df[['participant_id', 'source']]\n",
    "source_df = source_df.drop_duplicates()\n",
    "\n",
    "df_unique['participant_id'] = 'sub-ADNI' + df_unique['SubjectID_x'].str.replace('_', '')\n",
    "\n",
    "\n",
    "final = df_unique.merge(source_df, on='participant_id', how='left')\n",
    "final = final.drop_duplicates()\n",
    "\n",
    "final['source'] = final['source'].fillna('unseen')\n",
    "final = final.dropna()\n",
    "final = final[final['SubjectID_x'] != '133_S_0488'] #this is an outlier, conversion didn't work.\n",
    "\n",
    "\n",
    "#Now reorder and rename columns so that my teammates can more easily use this dataset\n",
    "\n",
    "cols = final.columns.tolist()\n",
    "cols = cols[-5:] + cols[:-5]  # Move the last five columns to the front\n",
    "final = final[cols]\n",
    "\n",
    "final = final.rename(columns = {'DXCURREN':'Diagnosis','SubjectID_x':'SubjectID','SessionId_x':'SessionId','ClosestDate':'DiagnosisVisDate'})\n",
    "\n",
    "cols = final.columns.tolist()\n",
    "newcols = ['scan_ID','Diagnosis','source','SubjectID','SessionId','DiagnosisVisDate'] +cols[7:]\n",
    "final = final[newcols]\n",
    "\n",
    "# Count the number of \"None\" values in the 'Column1'\n",
    "unknown_count = final['SessionId'].value_counts(dropna=False).get(\"Unknown\", 0)\n",
    "\n",
    "print(\"Number of 'None' values in 'Column1':\", unknown_count)\n",
    "\n",
    "final.to_csv('ADNI_Features_1-3v4.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85afe9-92ad-439e-bb04-2ee9177693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-SNE of MS / AD / Parkinson's Data\n",
    "\n",
    "#All imports\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import skimage.transform as skTrans\n",
    "import altair as alt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "#Initialize pretrained model\n",
    "device = torch.device('cpu')\n",
    "checkpoint = torch.load('age_expansion_8_model_low_loss.pth.tar', map_location=device)\n",
    "model_state_dict = checkpoint['state_dict']\n",
    "from models import NetWork\n",
    "model = NetWork(in_channel=3, feat_dim=1024) #should be 1024\n",
    "\n",
    "\n",
    "#Extract Features from all files in a folder.\n",
    "\n",
    "results = []\n",
    "root_path = 'T1_Files'\n",
    "\n",
    "# Need a regular expression to extract patient ID and diagnosis from filenames\n",
    "id_pattern = re.compile(r'(\\d{4})_\\d+?_([A-Z]{2})\\.nii')\n",
    "\n",
    "# Go through all files in the directory\n",
    "for file_name in os.listdir(root_path):\n",
    "    \n",
    "    # Use the regular expression to extract the patient ID and diagnosis from the filename\n",
    "    match = id_pattern.match(file_name)\n",
    "    \n",
    "    if match:\n",
    "        patient_id = int(match.group(1))  # Convert ID to integer to remove leading zeros\n",
    "        diagnosis = match.group(2)\n",
    "        \n",
    "        image_path = os.path.join(root_path, file_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = nib.load(image_path)\n",
    "        data = image.get_fdata()\n",
    "        \n",
    "        data = skTrans.resize(data, (3,100,100,100), order=1, preserve_range=True)\n",
    "        \n",
    "        data = (data - data.mean()) / data.std()\n",
    "        \n",
    "        # Convert to PyTorch tensor and adjust dimensions\n",
    "        data = torch.tensor(data, dtype=torch.float32).unsqueeze(0)\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        output = model(data, age_id=None).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        # Store results\n",
    "        for index, value in enumerate(output):\n",
    "            results.append({'PatientID': patient_id, 'Diagnosis': diagnosis, 'FeatureValue': value.item(), 'FeatureIndex': index})\n",
    "\n",
    "# Create a Pandas DataFrame from the dic of results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(df)\n",
    "df.to_excel(\"MS_AD_HC_Features.xlsx\")  #should specify index = False next time\n",
    "\n",
    "#Let's decide how many PCA components we want\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get one row per patient with all features\n",
    "df_pivot = df.pivot(index='PatientID', columns='FeatureIndex', values='FeatureValue')\n",
    "\n",
    "#get the info \n",
    "X = df_pivot.values\n",
    "\n",
    "# standardize the features\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA to  the scaled data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Create a dataframe w/ the cumulative explained variance\n",
    "explained_variance_df = pd.DataFrame({\n",
    "    'Component': range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "    'Cumulative Explained Variance': np.cumsum(pca.explained_variance_ratio_)\n",
    "})\n",
    "\n",
    "\n",
    "cumulative_variance_chart = alt.Chart(explained_variance_df).mark_line(point=True).encode(\n",
    "    alt.X('Component:O').title('Components'),\n",
    "    y='Cumulative Explained Variance:Q'\n",
    ").properties(\n",
    "    title='Cumulative Explained Variance by Different Principal Components'\n",
    ")\n",
    "\n",
    "cumulative_variance_chart.display()\n",
    "\n",
    "\n",
    "#Apply PCA!\n",
    "\n",
    "\n",
    "# Pivot the DataFrame to get one row per patient with 1024 feature columns\n",
    "features = df.pivot(index='PatientID', columns='FeatureIndex', values='FeatureValue')\n",
    "\n",
    "#features data frame now has one row per patient and 1024 columns for features\n",
    "\n",
    "# now standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Perform PCA\n",
    "n_components = 100  # The number of components to keep, based on above chart\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "principal_df = pd.DataFrame(data=principal_components,\n",
    "                            index=features.index,\n",
    "                            columns=[f'PC{i}' for i in range(1, n_components + 1)])\n",
    "\n",
    "# Reset index to bring PatientID back as a column\n",
    "principal_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# Now, extract the unique 'PatientID' and 'Diagnosis' from the original df\n",
    "patient_info = df[['PatientID', 'Diagnosis']].drop_duplicates().set_index('PatientID')\n",
    "\n",
    "# Merge the patient info and the principal components dataframes\n",
    "final_df = patient_info.join(principal_df.set_index('PatientID')).reset_index()\n",
    "\n",
    "print(final_df)\n",
    "principal_df.head()\n",
    "\n",
    "\n",
    "#Apply t-SNE!\n",
    "\n",
    "\n",
    "# Select the feature columns for t-SNE\n",
    "feature_columns = [f'PC{i}' for i in range(1, n_components + 1)]\n",
    "\n",
    "# Extract feature data\n",
    "feature_data = final_df[feature_columns]\n",
    "\n",
    "# Apply t-SNE \n",
    "n_components_tsne = 2  #2 components for viz \n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=42)\n",
    "tsne_results = tsne.fit_transform(feature_data)\n",
    "\n",
    "tsne_df = pd.DataFrame(data=tsne_results, columns=[f't-SNE{i}' for i in range(1, n_components_tsne + 1)])\n",
    "\n",
    "tsne_final_df = pd.concat([final_df[['PatientID', 'Diagnosis']].reset_index(drop=True), tsne_df], axis=1)\n",
    "\n",
    "scatter_plot = alt.Chart(tsne_final_df).mark_circle(size=60).encode(\n",
    "    x='t-SNE1',\n",
    "    y='t-SNE2',\n",
    "    color='Diagnosis:N',  # Color by Diagnosis\n",
    "    tooltip=['PatientID', 'Diagnosis']\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "scatter_plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae7fa4-cf07-4788-89d0-8b7492a04828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produces chart of model accuracy with different parameters (as high as 85%)\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from classifier import LinearClassifierAlexNet\n",
    "\n",
    "#Choose only ones not used for training\n",
    "\n",
    "ADNI_data = pd.read_csv('ADNI_Features_1-3v4.csv')\n",
    "df = ADNI_data[ADNI_data['source']!='train']\n",
    "df\n",
    "\n",
    "# Don't want non-numeric columns\n",
    "numeric_df = df.select_dtypes(include=[float, int]).dropna()\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.999)  # Preserve 99% of variance\n",
    "pca_result = pca.fit_transform(numeric_df)\n",
    "pca_df = pd.DataFrame(data=pca_result)\n",
    "pca_df = pd.concat([df['Diagnosis'], pca_df], axis=1)\n",
    "\n",
    "\n",
    "# Function to run the model with different parameters\n",
    "def run_model(X, y_encoded, loss_function, optimizer_function, lr, pca_components):\n",
    "    # PCA transformation\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Initialize the classifier\n",
    "    in_dim = X_train.shape[1]\n",
    "    model = LinearClassifierAlexNet(in_dim)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = loss_function()\n",
    "    optimizer = optimizer_function(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    epochsmax = 50\n",
    "\n",
    "\n",
    "    accuracy = []\n",
    "    for epoch in range(epochsmax):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            outputs = model(X_test)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_test.size(0)\n",
    "            correct += (predicted == y_test).sum().item()\n",
    "        accuracy.append(correct / total * 100)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Load and preprocess the data\n",
    "pca_df = pca_df.dropna()\n",
    "X = pca_df.drop(columns=['Diagnosis'])\n",
    "y = pca_df['Diagnosis']\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Parameters to try\n",
    "loss_functions = [torch.nn.CrossEntropyLoss, torch.nn.NLLLoss]\n",
    "optimizer_functions = [torch.optim.Adam, torch.optim.SGD]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "pca_components_list = [0.9, 0.95, 0.99, 0.999]\n",
    "\n",
    "# Running the model with different parameters\n",
    "results = []\n",
    "for loss_function in loss_functions:\n",
    "    for optimizer_function in optimizer_functions:\n",
    "        for lr in learning_rates:\n",
    "            for pca_components in pca_components_list:\n",
    "                accuracy = run_model(X, y_encoded, loss_function, optimizer_function, lr, pca_components)\n",
    "                for epoch, acc in enumerate(accuracy):\n",
    "                    results.append({\n",
    "                        'Epoch': epoch,\n",
    "                        'Accuracy': acc,\n",
    "                        'Loss Function': loss_function.__name__,\n",
    "                        'Optimizer': optimizer_function.__name__,\n",
    "                        'Learning Rate': lr,\n",
    "                        'PCA Components': pca_components\n",
    "                    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "chart = alt.Chart(results_df).mark_circle().encode(\n",
    "    x='Epoch:Q',\n",
    "    y=alt.Y('Accuracy:Q', scale=alt.Scale(domain=[0, 100])),\n",
    "    color='Learning Rate:N',\n",
    "    tooltip=['Epoch', 'Accuracy', 'Loss Function', 'Optimizer', 'Learning Rate', 'PCA Components']\n",
    ").properties(\n",
    "    width=180,\n",
    "    height=150\n",
    ")\n",
    "\n",
    "# Create small multiples\n",
    "chart = chart.facet(\n",
    "    column='Loss Function:N',\n",
    "    row='Optimizer:N'\n",
    ").resolve_scale(\n",
    "    x='independent',\n",
    "    y='independent'\n",
    ").properties(\n",
    "    title=\"Linear Classifier Accuracy\"\n",
    ")\n",
    "chart = chart.configure_title(\n",
    "    align='center'\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee831610-0bc4-4a85-8d2b-edfcd65fe4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Grid Search Compare AUC\n",
    "# @markdown Stochastic Gradient Descent w/ Cross Entropy Loss and batch size of 64. Compares for both LinearClassifierAlexNet and AdversarialClassifier from classifier.py number of hidden layers, learning rates, momentum, number of pca components, and number of epochs.\n",
    "# WARNING: RUN AT YOUR OWN RISK, TAKES 3-4HOURS\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Increase reproducibility\n",
    "seed = 20021102\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df = pd.read_csv('ADNI_Features_1-3v4.csv').dropna()\n",
    "df = df[(df['Diagnosis'] == 1.0) | (df['Diagnosis'] == 3.0)] #leave out MCI but left in model training data for now\n",
    "\n",
    "from classifier import AdversarialClassifier\n",
    "from classifier import LinearClassifierAlexNet\n",
    "\n",
    "Y = df['Diagnosis']\n",
    "X = df.drop(columns=['Diagnosis', 'scan_ID', 'source', 'SubjectID', 'SessionId', 'DiagnosisVisDate', 'UniqueID'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=42)\n",
    "\n",
    "#Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "#map diagnoses\n",
    "y_train = y_train.map({1.0: 0, 3.0: 1})\n",
    "y_val = y_val.map({1.0: 0, 3.0: 1})\n",
    "y_test = y_test.map({1.0: 0, 3.0: 1})\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "#DataLoader for validation\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=64)\n",
    "\n",
    "from classifier import AdversarialClassifier\n",
    "from classifier import LinearClassifierAlexNet\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_AdversarialClassifier = {\n",
    "    'nhid': [100, 200, 300, 400],\n",
    "    'lr': [0.0001, 0.001, 0.01],\n",
    "    'momentum': [0.8, 0.9, 0.93, 0.95, 0.99],\n",
    "    'num_epochs' : [100, 125, 150, 175, 200, 300, 400, 500],\n",
    "    'pca_components': [0.9, 0.95, 0.99, None]\n",
    "}\n",
    "\n",
    "param_grid_LinearClassifierAlexNet = {\n",
    "    'n_hid': [100, 200, 300, 400],\n",
    "    'lr': [0.0001, 0.001, 0.01],\n",
    "    'momentum': [0.8, 0.9, 0.93, 0.95, 0.99],\n",
    "    'num_epochs' : [100, 125, 150, 175, 200, 300, 400, 500],\n",
    "    'pca_components': [0.9, 0.95, 0.99, None]\n",
    "}\n",
    "\n",
    "def apply_pca(X_train, X_val, n_components):\n",
    "    if n_components is None:\n",
    "        return X_train, X_val  # No PCA applied\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)  # Transform validation data using training PCA fit\n",
    "    return X_train_pca, X_val_pca\n",
    "\n",
    "\n",
    "def train_evaluate_model(model, train_loader, val_loader, num_epochs, lr, momentum):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                probabilities = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(probabilities)\n",
    "\n",
    "                # Check for NaNs in predictions\n",
    "                if np.isnan(probabilities).any():\n",
    "                    raise ValueError(\"NaN found in model predictions\")\n",
    "\n",
    "    if np.isnan(all_labels).any():\n",
    "        raise ValueError(\"NaN found in validation labels\")\n",
    "\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    return auc_score\n",
    "\n",
    "\n",
    "def grid_search(param_grid, classifier_type, X_train, y_train, X_val, y_val):\n",
    "    best_auc = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for params in itertools.product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Apply PCA with the number of components set\n",
    "        X_train_pca, X_val_pca = apply_pca(X_train, X_val, param_dict['pca_components'])\n",
    "\n",
    "        # Adjust in_dim for the PCA-transformed data\n",
    "        in_dim = X_train_pca.shape[1]\n",
    "\n",
    "        # Make Dataloaders for the PCA data\n",
    "        train_data_pca = TensorDataset(torch.tensor(X_train_pca, dtype=torch.float32), y_train)\n",
    "        train_loader_pca = DataLoader(dataset=train_data_pca, batch_size=64, shuffle=True)\n",
    "        val_data_pca = TensorDataset(torch.tensor(X_val_pca, dtype=torch.float32), y_val)\n",
    "        val_loader_pca = DataLoader(dataset=val_data_pca, batch_size=64)\n",
    "\n",
    "        # Initialize the model with the adjusted in_dim\n",
    "        if classifier_type == 'AdversarialClassifier':\n",
    "            model = AdversarialClassifier(in_dim=in_dim, nhid=param_dict['nhid'], out_dim=2)\n",
    "        elif classifier_type == 'LinearClassifierAlexNet':\n",
    "            model = LinearClassifierAlexNet(in_dim=in_dim, n_hid=param_dict['n_hid'], n_label=2)\n",
    "\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        auc_score = train_evaluate_model(model, train_loader_pca, val_loader_pca, param_dict['num_epochs'], param_dict['lr'], param_dict['momentum'])\n",
    "        print(auc_score)\n",
    "\n",
    "\n",
    "        if auc_score > best_auc:\n",
    "            best_auc = auc_score\n",
    "            best_params = param_dict\n",
    "            \n",
    "\n",
    "    return best_params, best_auc\n",
    "\n",
    "best_params_AdversarialClassifier, best_auc_AdversarialClassifier = grid_search(\n",
    "    param_grid_AdversarialClassifier, 'AdversarialClassifier', X_train.numpy(), y_train, X_val.numpy(), y_val)\n",
    "\n",
    "best_params_LinearClassifierAlexNet, best_auc_LinearClassifierAlexNet = grid_search(\n",
    "    param_grid_LinearClassifierAlexNet, 'LinearClassifierAlexNet', X_train.numpy(), y_train, X_val.numpy(), y_val)\n",
    "\n",
    "print(\"Best Params for AdversarialClassifier:\", best_params_AdversarialClassifier, \"with AUC:\", best_auc_AdversarialClassifier)\n",
    "print(\"Best Params for LinearClassifierAlexNet:\", best_params_LinearClassifierAlexNet, \"with AUC:\", best_auc_LinearClassifierAlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c7692-7dec-4420-886a-bb9c70234d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title AUC\n",
    "# @markdown let's try some more combinations. and also let's increase test set size ... hopefully that was the issue and not overfitting :)\n",
    "\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "#Increase reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df = pd.read_csv('ADNI_Features_1-3v4.csv').dropna()\n",
    "df = df[(df['Diagnosis'] == 1.0) | (df['Diagnosis'] == 3.0)] #leave out MCI but left in model training data for now\n",
    "#df = df[df['source']!='train'] #take out model training data but leave val (not ideal)\n",
    "\n",
    "from classifier import AdversarialClassifier\n",
    "from classifier import LinearClassifierAlexNet\n",
    "\n",
    "Y = df['Diagnosis']\n",
    "X = df.drop(columns=['Diagnosis', 'scan_ID', 'source', 'SubjectID', 'SessionId', 'DiagnosisVisDate', 'UniqueID'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "#map diagnoses\n",
    "y_train = y_train.map({1.0: 0, 3.0: 1})\n",
    "y_val = y_val.map({1.0: 0, 3.0: 1})\n",
    "y_test = y_test.map({1.0: 0, 3.0: 1})\n",
    "\n",
    "\n",
    "# Convert to pytorch tensors\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "#DataLoader for validation\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_AdversarialClassifier = {\n",
    "    #'nhid': [200, 300, 400, 500],\n",
    "    'nhid': [400, 500],\n",
    "    'lr': [0.001, 0.01],\n",
    "    #'momentum': [0.9, 0.93, 0.95, 0.99],\n",
    "    'momentum': [0.93, 0.99],\n",
    "    'num_epochs' : [60, 500, 600], #500 epochs was best for both classifiers earlier, maybe higher would be even better?\n",
    "    #'pca_components': [0.9, 0.95, 0.99, None]\n",
    "    'pca_components': [0.95]\n",
    "}\n",
    "\n",
    "param_grid_LinearClassifierAlexNet = {\n",
    "    #'n_hid': [200, 400, 500],\n",
    "    'n_hid': [400, 500],\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'momentum': [0.93, 0.99],\n",
    "    'num_epochs' : [60, 500, 600],\n",
    "    #'pca_components': [0.9, 0.95, 0.99, None]\n",
    "    'pca_components': [0.95]\n",
    "}\n",
    "\n",
    "def apply_pca(X_train, X_val, n_components):\n",
    "    if n_components is None:\n",
    "        return X_train, X_val  # No PCA applied\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)  # Transform validation data using training PCA fit\n",
    "    return X_train_pca, X_val_pca\n",
    "\n",
    "\n",
    "def train_evaluate_model(model, train_loader, val_loader, num_epochs, lr, momentum):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                probabilities = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(probabilities)\n",
    "\n",
    "                # Check for nans in predictions\n",
    "                if np.isnan(probabilities).any():\n",
    "                    raise ValueError(\"NaN found in model predictions\")\n",
    "    #and labels\n",
    "    if np.isnan(all_labels).any():\n",
    "        raise ValueError(\"NaN found in validation labels\")\n",
    "\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    return auc_score\n",
    "\n",
    "\n",
    "def grid_search(param_grid, classifier_type, X_train, y_train, X_val, y_val):\n",
    "    best_auc = 0\n",
    "    k = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for params in itertools.product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Apply PCA w the number of components set\n",
    "        X_train_pca, X_val_pca = apply_pca(X_train, X_val, param_dict['pca_components'])\n",
    "\n",
    "        # need to adjust in_dim for the PCA-transformed data\n",
    "        in_dim = X_train_pca.shape[1]\n",
    "\n",
    "        # Make dataloaders for the PCA data\n",
    "        train_data_pca = TensorDataset(torch.tensor(X_train_pca, dtype=torch.float32), y_train)\n",
    "        train_loader_pca = DataLoader(dataset=train_data_pca, batch_size=4, shuffle=True) #batch size of 4 this time let's see if that's better\n",
    "        val_data_pca = TensorDataset(torch.tensor(X_val_pca, dtype=torch.float32), y_val)\n",
    "        val_loader_pca = DataLoader(dataset=val_data_pca, batch_size=4)\n",
    "\n",
    "        # Initialize  model with the adjusted in_dim\n",
    "        if classifier_type == 'AdversarialClassifier':\n",
    "            model = AdversarialClassifier(in_dim=in_dim, nhid=param_dict['nhid'], out_dim=2)\n",
    "        elif classifier_type == 'LinearClassifierAlexNet':\n",
    "            model = LinearClassifierAlexNet(in_dim=in_dim, n_hid=param_dict['n_hid'], n_label=2)\n",
    "\n",
    "\n",
    "        # Train and eval the model\n",
    "        auc_score = train_evaluate_model(model, train_loader_pca, val_loader_pca, param_dict['num_epochs'], param_dict['lr'], param_dict['momentum'])\n",
    "        k += 1\n",
    "        print(auc_score, best_auc, k, param_dict)\n",
    "\n",
    "\n",
    "        if auc_score > best_auc:\n",
    "            best_auc = auc_score\n",
    "            best_params = param_dict\n",
    "            \n",
    "\n",
    "    return best_params, best_auc\n",
    "\n",
    "\n",
    "best_params_LinearClassifierAlexNet, best_auc_LinearClassifierAlexNet = grid_search(\n",
    "    param_grid_LinearClassifierAlexNet, 'LinearClassifierAlexNet', X_train.numpy(), y_train, X_val.numpy(), y_val)\n",
    "\n",
    "best_params_AdversarialClassifier, best_auc_AdversarialClassifier = grid_search(\n",
    "    param_grid_AdversarialClassifier, 'AdversarialClassifier', X_train.numpy(), y_train, X_val.numpy(), y_val)\n",
    "\n",
    "\n",
    "print(\"Best Params for AdversarialClassifier:\", best_params_AdversarialClassifier, \"with AUC:\", best_auc_AdversarialClassifier)\n",
    "print(\"Best Params for LinearClassifierAlexNet:\", best_params_LinearClassifierAlexNet, \"with AUC:\", best_auc_LinearClassifierAlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad925e9-c915-4205-a618-108058b34876",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC CALCULATION\n",
    "\n",
    "#best_params_AdversarialClassifier =  {'nhid': 300, 'lr': 0.001, 'momentum': 0.93, 'num_epochs': 500, 'pca_components': 0.99} \n",
    "#best_params_LinearClassifierAlexNet=  {'n_hid': 100, 'lr': 0.01, 'momentum': 0.99, 'num_epochs': 500, 'pca_components': 0.9} \n",
    " \n",
    "\n",
    "#USING PARAMETERS FROM THE PAPER OF THE CNN (WHICH ACHIEVED 85.12 AUC):\n",
    "best_params_AdversarialClassifier =  {'nhid': 200, 'lr': 0.01, 'momentum': 0.9, 'num_epochs': 60, 'pca_components': 0.95} \n",
    "best_params_LinearClassifierAlexNet=  {'n_hid': 200, 'lr': 0.01, 'momentum': 0.9, 'num_epochs': 60, 'pca_components': 0.95} \n",
    " \n",
    "\n",
    "#Evaluate test set with best parameters based on grid search\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_test_set(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(probabilities)\n",
    "\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    return auc_score\n",
    "\n",
    "# Apply PCA to the test set\n",
    "X_train_pca, X_test_pca = apply_pca(X_train.numpy(), X_test.numpy(), best_params_AdversarialClassifier['pca_components'])\n",
    "\n",
    "# Convert test set to PyTorch tensors\n",
    "test_data = TensorDataset(torch.tensor(X_test_pca, dtype=torch.float32), y_test)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64)\n",
    "\n",
    "# Initialize models with best parameters\n",
    "best_model_AdversarialClassifier = AdversarialClassifier(in_dim=X_test_pca.shape[1], nhid=best_params_AdversarialClassifier['nhid'], out_dim=2)\n",
    "best_model_LinearClassifierAlexNet = LinearClassifierAlexNet(in_dim=X_test_pca.shape[1], n_hid=best_params_LinearClassifierAlexNet['n_hid'], n_label=2)\n",
    "\n",
    "# Evaluate AdversarialClassifier on test set\n",
    "test_auc_AdversarialClassifier = evaluate_test_set(best_model_AdversarialClassifier, test_loader)\n",
    "print(\"Test AUC for AdversarialClassifier:\", test_auc_AdversarialClassifier)\n",
    "\n",
    "# Evaluate LinearClassifierAlexNet on test set\n",
    "test_auc_LinearClassifierAlexNet = evaluate_test_set(best_model_LinearClassifierAlexNet, test_loader)\n",
    "print(\"Test AUC for LinearClassifierAlexNet:\", test_auc_LinearClassifierAlexNet)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
